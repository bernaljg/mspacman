{
 "metadata": {
  "name": "",
  "signature": "sha256:0822979f583c40d9948d82f4b5eb996af8cf21bdaee4ff53eddaef015e8c14d3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import gym"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Testing Document\n",
      "\n",
      "Fully connected two layer network to perform Q-learning for pacman\n",
      "\n",
      "The basic relevant features for the game state are:\n",
      "-MsPacman's location: 2 values\n",
      "-Four pairs of distances from each ghost: 8 values\n",
      "    \n",
      "Total of 10 input values.\n",
      "*If there are less ghosts we can set an arbitrarily high number for their distances.\n",
      "The hope is that the map will be encoded automatically by the neural network structure.\n",
      "\n",
      "The network will compute Q values for each action \"a\" from a state \"s\". We will then choose an action. \n",
      "\"\"\"\n",
      "\n",
      "def model():\n",
      "    env = gym.make('MsPacman-v0')\n",
      "\n",
      "    game_state = tf.placeholder(\"float32\",[10])\n",
      "\n",
      "    w1_init = tf.random_normal((100,10), name = \"w1_init\")\n",
      "    w2_init = tf.random_normal((9,100), name = \"w2_init\")\n",
      "\n",
      "    w1 = tf.Variable(w1_init, name=\"w1\")\n",
      "    w2 = tf.Variable(w2_init, name=\"w2\")\n",
      "\n",
      "    y1 = tf.matmul(w1,state)\n",
      "    layer1_act = tf.nn.relu(y1)\n",
      "\n",
      "    y2 = tf.matmul(w2,layer1_act)\n",
      "    Q_vals = tf.nn.log_softmax(y2)\n",
      "\n",
      "    action = tf.arg_max(Q_vals)\n",
      "\n",
      "    obs,_,reward,done = env.step(action)\n",
      "\n",
      "    new_state = extract_features(obs)\n",
      "\n",
      "    new_y1 = tf.matmul(w1,new_state)\n",
      "    new_layer1_act = tf.nn.relu(new_y1)\n",
      "\n",
      "    new_y2 = tf.matmul(w2,new_layer1_act)\n",
      "    new_Q_vals = tf.nn.log_softmax(new_y2)\n",
      "    \n",
      "    max_Q = tf.maximum(new_Q_vals)    \n",
      "    \n",
      "    new_Q_vals = np.copy(Q_vals)\n",
      "    new_Q_Vals[action] = reward + max_Q\n",
      "    \n",
      "    loss = tf.nn.l2_loss((new_Q_vals - Q_vals))\n",
      "    \n",
      "    tf.train.GradientDescentOptimizer.minimize(loss)\n",
      "\n",
      "\n",
      "tf.constant(0.001)\n",
      "tf.random_uniform([1],minval=0,maxval=9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<tf.Tensor 'random_uniform_1:0' shape=(1,) dtype=float32>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "'L2 Loss.\\n\\n  Computes half the L2 norm of a tensor without the `sqrt`:\\n\\n      output = sum(t ** 2) / 2\\n\\n  Args:\\n    t: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`, `complex128`, `qint8`, `quint8`, `qint32`, `half`.\\n      Typically 2-D, but may have any dimensions.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `Tensor`. Has the same type as `t`. 0-D.\\n  '"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf.to_int32"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}